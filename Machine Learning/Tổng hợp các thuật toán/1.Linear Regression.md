# Linear Regression (Hồi Quy Tuyến Tính)

## 1. Giới Thiệu

Hồi quy tuyến tính (Linear Regression) là một trong những thuật toán Machine Learning cơ bản và quan trọng nhất, được sử dụng để giải quyết các bài toán hồi quy (regression). Mục tiêu của thuật toán là dự đoán các giá trị liên tục dựa trên mối quan hệ tuyến tính giữa biến đầu vào và biến đầu ra.

## 2. Bản Chất

Linear Regression tìm kiếm mối quan hệ tuyến tính (quan hệ bậc nhất) giữa một hoặc nhiều biến đầu vào **x** (features) và biến đầu ra **y** (target). Mô hình cố gắng xấp xỉ dữ liệu bằng một hàm tuyến tính:

**Công thức tổng quát:**
```
y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
```

Hoặc dạng vector:
```
y = w₀ + wᵀx
```

Trong đó:
- **y**: Giá trị dự đoán (predicted value)
- **x**: Vector đặc trưng đầu vào (feature vector)
- **w**: Vector trọng số (weight vector)
- **w₀**: Hệ số chặn (bias/intercept)

## 3. Phân Loại

### 3.1. Hồi Quy Tuyến Tính Đơn Giản (Simple Linear Regression)

Chỉ có **một biến đầu vào** x duy nhất.

**Công thức:**
```
y = w₀ + w₁x
```

Mô hình được biểu diễn bằng một đường thẳng trong không gian 2 chiều.

**Ví dụ:** Dự đoán mức lương dựa trên số năm kinh nghiệm.

### 3.2. Hồi Quy Tuyến Tính Đa Biến (Multiple Linear Regression)

Có **nhiều hơn một biến đầu vào** (x₁, x₂, x₃, ..., xₙ).

**Công thức:**
```
y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
```

Mô hình được biểu diễn bằng một siêu phẳng (hyperplane) trong không gian nhiều chiều.

**Ví dụ:** Dự đoán giá nhà dựa trên diện tích, số phòng ngủ, vị trí, năm xây dựng.

## 4. Thuật Ngữ Quan Trọng

- **Biến độc lập (Independent Variable/Feature)**: Các biến đầu vào **x** được sử dụng để dự đoán.
- **Biến phụ thuộc (Dependent Variable/Target)**: Biến đầu ra **y** cần được dự đoán.
- **Trọng số (Weights)**: Các hệ số **w** thể hiện mức độ ảnh hưởng của từng feature.
- **Bias (Intercept)**: Hệ số **w₀** cho phép đường thẳng không nhất thiết phải đi qua gốc tọa độ.

## 5. Hàm Mất Mát (Loss Function)

### 5.1. Mean Squared Error (MSE)

Hàm mất mát phổ biến nhất cho Linear Regression:

```
MSE = (1/n) Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²
```

Trong đó:
- **n**: Số lượng mẫu dữ liệu
- **yᵢ**: Giá trị thực tế
- **ŷᵢ**: Giá trị dự đoán

### 5.2. Root Mean Squared Error (RMSE)

```
RMSE = √MSE
```

RMSE có cùng đơn vị với biến mục tiêu, dễ hiểu hơn MSE.

### 5.3. Mean Absolute Error (MAE)

```
MAE = (1/n) Σᵢ₌₁ⁿ |yᵢ - ŷᵢ|
```

MAE ít nhạy cảm với outliers hơn MSE.

## 6. Phương Pháp Tối Ưu

### 6.1. Normal Equation (Phương Trình Chuẩn)

Tìm nghiệm trực tiếp bằng công thức đại số:

```
w = (XᵀX)⁻¹Xᵀy
```

**Ưu điểm:**
- Tìm nghiệm chính xác trong một bước
- Không cần điều chỉnh learning rate

**Nhược điểm:**
- Tính toán chậm với dữ liệu lớn (độ phức tạp O(n³))
- Không khả thi khi XᵀX không khả nghịch

### 6.2. Gradient Descent (Hạ Gradient)

Tối ưu hóa lặp đi lặp lại bằng cách cập nhật trọng số theo hướng giảm gradient:

```
w := w - α · ∇J(w)
```

Trong đó:
- **α**: Learning rate (tốc độ học)
- **∇J(w)**: Gradient của hàm mất mát

**Các biến thể:**
- **Batch Gradient Descent**: Sử dụng toàn bộ dữ liệu
- **Stochastic Gradient Descent (SGD)**: Sử dụng từng mẫu
- **Mini-batch Gradient Descent**: Sử dụng một batch nhỏ

## 7. Đánh Giá Mô Hình

### 7.1. R² Score (Coefficient of Determination)

```
R² = 1 - (SS_res / SS_tot)
```

Trong đó:
- **SS_res**: Tổng bình phương phần dư
- **SS_tot**: Tổng bình phương

**Giá trị R²:**
- R² = 1: Mô hình hoàn hảo
- R² = 0: Mô hình không tốt hơn dự đoán trung bình
- R² < 0: Mô hình tệ hơn dự đoán trung bình

### 7.2. Adjusted R²

Điều chỉnh R² để tránh overfitting khi thêm nhiều features:

```
Adjusted R² = 1 - [(1-R²)(n-1)/(n-p-1)]
```

Trong đó:
- **n**: Số mẫu
- **p**: Số features

## 8. Giả Định Của Linear Regression

Để mô hình hoạt động tốt, cần thỏa mãn các giả định:

1. **Tính tuyến tính**: Mối quan hệ giữa x và y là tuyến tính
2. **Độc lập**: Các quan sát độc lập với nhau
3. **Homoscedasticity**: Phương sai của phần dư không đổi
4. **Phân phối chuẩn của phần dư**: Phần dư tuân theo phân phối chuẩn
5. **Không có multicollinearity**: Các features không tương quan cao với nhau

## 9. Ví Dụ Thực Tế

### Ví dụ 1: Dự đoán mức lương
```
Lương = w₀ + w₁ × (Số năm kinh nghiệm)
```

### Ví dụ 2: Dự đoán giá nhà
```
Giá nhà = w₀ + w₁×(Diện tích) + w₂×(Số phòng) + w₃×(Tuổi nhà)
```

### Ví dụ 3: Dự đoán điểm thi
```
Điểm thi = w₀ + w₁×(Giờ học) + w₂×(Điểm trước đó)
```

## 10. Ưu Điểm

✓ Đơn giản, dễ hiểu và dễ triển khai
✓ Tính toán nhanh, hiệu quả với dữ liệu lớn
✓ Dễ diễn giải kết quả (interpretability cao)
✓ Ít tham số cần điều chỉnh
✓ Hoạt động tốt khi mối quan hệ là tuyến tính

## 11. Nhược Điểm

✗ Chỉ mô hình hóa được mối quan hệ tuyến tính
✗ Nhạy cảm với outliers
✗ Giả định các features độc lập (multicollinearity gây vấn đề)
✗ Không phù hợp với dữ liệu phức tạp, phi tuyến
✗ Có thể bị underfitting với dữ liệu phức tạp

## 12. Cải Tiến và Biến Thể

### 12.1. Polynomial Regression (Hồi Quy Đa Thức)

Mở rộng Linear Regression để mô hình hóa mối quan hệ phi tuyến:

```
y = w₀ + w₁x + w₂x² + w₃x³ + ...
```

### 12.2. Ridge Regression (L2 Regularization)

Thêm penalty term để tránh overfitting:

```
J(w) = MSE + λ Σwᵢ²
```

### 12.3. Lasso Regression (L1 Regularization)

Thêm penalty term và có khả năng feature selection:

```
J(w) = MSE + λ Σ|wᵢ|
```

### 12.4. Elastic Net

Kết hợp cả L1 và L2 regularization:

```
J(w) = MSE + λ₁ Σ|wᵢ| + λ₂ Σwᵢ²
```

## 13. Khi Nào Sử Dụng Linear Regression?

✓ Khi mối quan hệ giữa input và output là tuyến tính hoặc gần tuyến tính
✓ Khi cần mô hình đơn giản, dễ diễn giải
✓ Khi dữ liệu không có quá nhiều outliers
✓ Khi số lượng features không quá lớn so với số mẫu
✓ Làm baseline model trước khi thử các mô hình phức tạp hơn

## 14. Triển Khai Với Python

### Sử dụng Scikit-learn:

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Chia dữ liệu
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Khởi tạo và huấn luyện mô hình
model = LinearRegression()
model.fit(X_train, y_train)

# Dự đoán
y_pred = model.predict(X_test)

# Đánh giá
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse}")
print(f"R² Score: {r2}")
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
```

## 15. Tài Liệu Tham Khảo

- Bishop, C. M. (2006). Pattern Recognition and Machine Learning
- Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective
- Vũ Hữu Tiệp. Machine Learning Cơ Bản

---

## Câu Hỏi Thường Gặp

**Q1: Sự khác biệt giữa hồi quy tuyến tính và hồi quy đa thức là gì?**

A: Hồi quy tuyến tính mô hình hóa mối quan hệ bậc nhất (đường thẳng), trong khi hồi quy đa thức có thể mô hình hóa mối quan hệ phi tuyến bằng cách sử dụng các lũy thừa của biến đầu vào (x², x³, ...).

**Q2: Làm thế nào để tránh hiện tượng overfitting trong mô hình hồi quy?**

A: Có thể sử dụng:
- Regularization (Ridge, Lasso, Elastic Net)
- Cross-validation để đánh giá mô hình
- Giảm số lượng features (feature selection)
- Thu thập thêm dữ liệu training
- Early stopping trong gradient descent

**Q3: Hàm mất mát Mean Squared Error hoạt động như thế nào?**

A: MSE tính trung bình của bình phương sai số giữa giá trị thực tế và giá trị dự đoán. Việc bình phương làm cho các sai số lớn bị phạt nặng hơn, giúp mô hình tập trung vào việc giảm thiểu các sai số lớn. MSE luôn không âm và bằng 0 khi dự đoán hoàn hảo.
