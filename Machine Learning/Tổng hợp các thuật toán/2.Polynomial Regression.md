# Polynomial Regression (Hồi Quy Đa Thức)

## 1. Giới Thiệu

Hồi quy đa thức (Polynomial Regression) là một phiên bản mở rộng của hồi quy tuyến tính, được sử dụng để mô hình hóa các mối quan hệ phi tuyến phức tạp giữa biến đầu vào và biến đầu ra. Mặc dù có tên gọi là "đa thức", về bản chất đây vẫn là một dạng của hồi quy tuyến tính nhưng với các features được biến đổi thành các lũy thừa.

## 2. Bản Chất

Polynomial Regression mở rộng Linear Regression bằng cách thêm các hạng tử bậc cao (x², x³, ..., xⁿ) vào phương trình, cho phép mô hình hóa các mối quan hệ cong, phi tuyến.

**Công thức tổng quát:**
```
y = w₀ + w₁x + w₂x² + w₃x³ + ... + wₙxⁿ
```

Hoặc dạng tổng quát:
```
y = Σᵢ₌₀ⁿ wᵢxⁱ
```

Trong đó:
- **y**: Giá trị dự đoán
- **x**: Biến đầu vào
- **wᵢ**: Các hệ số (trọng số)
- **n**: Bậc của đa thức (degree)

## 3. Sự Khác Biệt Với Linear Regression

| Đặc điểm | Linear Regression | Polynomial Regression |
|----------|-------------------|----------------------|
| Công thức | y = w₀ + w₁x | y = w₀ + w₁x + w₂x² + ... + wₙxⁿ |
| Đồ thị | Đường thẳng | Đường cong |
| Độ phức tạp | Thấp | Cao hơn (phụ thuộc vào n) |
| Khả năng mô hình hóa | Chỉ quan hệ tuyến tính | Quan hệ phi tuyến |
| Nguy cơ overfitting | Thấp | Cao (với n lớn) |

**Lưu ý quan trọng:** Mặc dù mô hình hóa quan hệ phi tuyến, Polynomial Regression vẫn là "tuyến tính" theo các hệ số w, do đó có thể sử dụng các phương pháp tối ưu giống Linear Regression.

## 4. Bậc Của Đa Thức (Degree)

### 4.1. Các Bậc Phổ Biến

**Bậc 1 (n=1):** Linear Regression
```
y = w₀ + w₁x
```

**Bậc 2 (n=2):** Quadratic Regression
```
y = w₀ + w₁x + w₂x²
```
Tạo ra đường parabol, phù hợp với dữ liệu có một điểm cực trị.

**Bậc 3 (n=3):** Cubic Regression
```
y = w₀ + w₁x + w₂x² + w₃x³
```
Có thể có hai điểm cực trị, mô hình hóa dữ liệu phức tạp hơn.

**Bậc cao hơn (n>3):**
Càng linh hoạt nhưng càng dễ bị overfitting.

### 4.2. Cách Chọn Bậc n

Việc chọn bậc n là một quyết định quan trọng:

✓ **n quá thấp**: Underfitting - mô hình quá đơn giản, không nắm bắt được pattern
✗ **n quá cao**: Overfitting - mô hình quá phức tạp, học thuộc dữ liệu training

**Phương pháp chọn n:**
1. Sử dụng Cross-Validation để thử nghiệm các giá trị n khác nhau
2. Vẽ biểu đồ Learning Curve
3. So sánh train error và validation error
4. Sử dụng các tiêu chí như AIC, BIC
5. Bắt đầu với n nhỏ và tăng dần

## 5. Polynomial Features (Đặc Trưng Đa Thức)

### 5.1. Biến Đổi Features

Polynomial Regression hoạt động bằng cách tạo ra các features mới từ features gốc:

**Input gốc:**
```
x = [x₁]
```

**Polynomial features (degree=2):**
```
[1, x₁, x₁²]
```

**Polynomial features (degree=3):**
```
[1, x₁, x₁², x₁³]
```

### 5.2. Đa Biến (Multiple Variables)

Với nhiều biến đầu vào, polynomial features bao gồm cả các tương tác:

**Input gốc:**
```
x = [x₁, x₂]
```

**Polynomial features (degree=2):**
```
[1, x₁, x₂, x₁², x₁x₂, x₂²]
```

**Polynomial features (degree=3):**
```
[1, x₁, x₂, x₁², x₁x₂, x₂², x₁³, x₁²x₂, x₁x₂², x₂³]
```

**Số lượng features:** Với d biến và bậc n, số features mới là:
```
C(n+d, d) = (n+d)! / (n! × d!)
```

## 6. Vấn Đề Overfitting

### 6.1. Nguyên Nhân

Overfitting xảy ra khi:
- Bậc đa thức n quá cao so với độ phức tạp thực của dữ liệu
- Số lượng features nhiều hơn số lượng mẫu dữ liệu
- Mô hình học thuộc noise trong dữ liệu training

### 6.2. Dấu Hiệu Nhận Biết

✗ Train error rất thấp nhưng validation/test error cao
✗ Đường cong dự đoán dao động mạnh giữa các điểm dữ liệu
✗ Hệ số R² trên training set gần 1 nhưng trên test set thấp
✗ Các hệ số w có giá trị tuyệt đối rất lớn

### 6.3. Ví Dụ Minh Họa

Giả sử có 6 điểm dữ liệu:

**Degree = 2:** Đường cong mượt mà, khớp hợp lý
```
MSE_train = 0.15, MSE_test = 0.18
```

**Degree = 10:** Đi qua tất cả các điểm nhưng dao động mạnh
```
MSE_train = 0.001, MSE_test = 5.42
```

## 7. Kỹ Thuật Regularization

### 7.1. Ridge Regression (L2 Regularization)

Thêm penalty term để kiểm soát độ lớn của các hệ số:

```
J(w) = MSE + λ Σᵢ₌₁ⁿ wᵢ²
```

**Đặc điểm:**
- Giảm độ lớn của các hệ số nhưng không đưa về 0
- Phù hợp khi tất cả features đều có ý nghĩa
- Tham số λ (alpha) điều chỉnh mức độ regularization

### 7.2. Lasso Regression (L1 Regularization)

```
J(w) = MSE + λ Σᵢ₌₁ⁿ |wᵢ|
```

**Đặc điểm:**
- Có thể đưa một số hệ số về 0 (feature selection)
- Tạo ra mô hình sparse (thưa)
- Hữu ích khi có nhiều features không quan trọng

### 7.3. Elastic Net

Kết hợp cả L1 và L2:

```
J(w) = MSE + λ₁ Σ|wᵢ| + λ₂ Σwᵢ²
```

**Đặc điểm:**
- Cân bằng giữa Ridge và Lasso
- Xử lý tốt khi có nhiều features tương quan

### 7.4. So Sánh Regularization

| Phương pháp | Penalty | Feature Selection | Khi nào dùng |
|-------------|---------|-------------------|--------------|
| Ridge | L2 (Σw²) | Không | Tất cả features quan trọng |
| Lasso | L1 (Σ\|w\|) | Có | Nhiều features không quan trọng |
| Elastic Net | L1 + L2 | Có | Features tương quan cao |

## 8. Hàm Mất Mát

Giống như Linear Regression, Polynomial Regression sử dụng:

### 8.1. Mean Squared Error (MSE)
```
MSE = (1/n) Σᵢ₌₁ⁿ (yᵢ - ŷᵢ)²
```

### 8.2. MSE với Regularization

**Ridge:**
```
J(w) = (1/n) Σ(yᵢ - ŷᵢ)² + λ Σwᵢ²
```

**Lasso:**
```
J(w) = (1/n) Σ(yᵢ - ŷᵢ)² + λ Σ|wᵢ|
```

## 9. Đánh Giá Mô Hình

### 9.1. Metrics

- **R² Score**: Đo lường mức độ fit của mô hình
- **Adjusted R²**: Điều chỉnh cho số lượng features
- **MSE/RMSE**: Đo lường sai số trung bình
- **MAE**: Ít nhạy cảm với outliers

### 9.2. Cross-Validation

Sử dụng k-fold cross-validation để:
- Đánh giá hiệu suất tổng quát
- Chọn bậc n tối ưu
- Chọn tham số regularization λ

### 9.3. Learning Curves

Vẽ đồ thị train error và validation error theo:
- Số lượng mẫu training
- Bậc của đa thức
- Giá trị λ

## 10. Ví Dụ Thực Tế

### Ví dụ 1: Tăng trưởng dân số
```
Dân số = w₀ + w₁×năm + w₂×năm²
```
Mô hình hóa tăng trưởng theo hàm bậc 2.

### Ví dụ 2: Quỹ đạo vật thể
```
Độ cao = w₀ + w₁×thời_gian + w₂×thời_gian²
```
Mô phỏng chuyển động parabol.

### Ví dụ 3: Hiệu suất động cơ
```
Công suất = w₀ + w₁×tốc_độ + w₂×tốc_độ² + w₃×tốc_độ³
```
Quan hệ phi tuyến phức tạp.

### Ví dụ 4: Giá cổ phiếu theo thời gian
```
Giá = w₀ + w₁×t + w₂×t² + w₃×t³
```
Nắm bắt xu hướng và biến động.

## 11. Ưu Điểm

✓ Mô hình hóa được các mối quan hệ phi tuyến phức tạp
✓ Linh hoạt, có thể điều chỉnh độ phức tạp qua bậc n
✓ Vẫn sử dụng được các phương pháp tối ưu của Linear Regression
✓ Dễ triển khai và hiểu
✓ Không cần nhiều dữ liệu như neural networks
✓ Có thể kết hợp với regularization để tránh overfitting

## 12. Nhược Điểm

✗ Dễ bị overfitting với bậc cao
✗ Nhạy cảm với outliers
✗ Số lượng features tăng nhanh với bậc và số biến
✗ Khó diễn giải khi bậc cao
✗ Không phù hợp với dữ liệu có nhiều chiều (curse of dimensionality)
✗ Cần chuẩn hóa dữ liệu (feature scaling) để tránh numerical instability
✗ Extrapolation (dự đoán ngoài phạm vi training) thường không chính xác

## 13. Feature Scaling

### 13.1. Tại Sao Cần Scaling?

Với polynomial features, giá trị có thể trở nên rất lớn:
```
x = 100 → x² = 10,000 → x³ = 1,000,000
```

Điều này gây ra:
- Numerical instability
- Gradient descent hội tụ chậm
- Các features có scale khác nhau ảnh hưởng không đồng đều

### 13.2. Phương Pháp Scaling

**Standardization (Z-score normalization):**
```
x_scaled = (x - μ) / σ
```

**Min-Max Scaling:**
```
x_scaled = (x - min) / (max - min)
```

**Robust Scaling:**
```
x_scaled = (x - median) / IQR
```

## 14. Khi Nào Sử Dụng Polynomial Regression?

✓ Khi dữ liệu có xu hướng cong, phi tuyến rõ ràng
✓ Khi Linear Regression cho kết quả kém (R² thấp)
✓ Khi biết trước mối quan hệ có dạng đa thức (vật lý, hóa học)
✓ Khi cần mô hình đơn giản hơn neural networks
✓ Khi số lượng features không quá nhiều
✓ Khi có đủ dữ liệu để tránh overfitting

✗ Không nên dùng khi:
- Dữ liệu có nhiều chiều (>10 features)
- Mối quan hệ quá phức tạp, không có dạng đa thức
- Cần extrapolation xa khỏi phạm vi training
- Có quá ít dữ liệu so với độ phức tạp

## 15. Triển Khai Với Python

### 15.1. Sử Dụng Scikit-learn

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Tạo dữ liệu mẫu
np.random.seed(42)
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = 2 + 3*X + 0.5*X**2 + np.random.randn(100, 1) * 2

# Chia dữ liệu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Tạo pipeline với polynomial features
degree = 2
model = Pipeline([
    ('poly_features', PolynomialFeatures(degree=degree)),
    ('linear_regression', LinearRegression())
])

# Huấn luyện
model.fit(X_train, y_train)

# Dự đoán
y_pred = model.predict(X_test)

# Đánh giá
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Degree: {degree}")
print(f"MSE: {mse:.4f}")
print(f"R² Score: {r2:.4f}")

# Visualize
X_plot = np.linspace(0, 10, 100).reshape(-1, 1)
y_plot = model.predict(X_plot)

plt.scatter(X_train, y_train, alpha=0.5, label='Training data')
plt.scatter(X_test, y_test, alpha=0.5, label='Test data')
plt.plot(X_plot, y_plot, 'r-', label=f'Polynomial (degree={degree})')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('Polynomial Regression')
plt.show()
```

### 15.2. So Sánh Các Bậc Khác Nhau

```python
from sklearn.model_selection import cross_val_score

degrees = [1, 2, 3, 4, 5, 10, 15]
train_scores = []
test_scores = []

for degree in degrees:
    model = Pipeline([
        ('poly_features', PolynomialFeatures(degree=degree)),
        ('linear_regression', LinearRegression())
    ])
    
    model.fit(X_train, y_train)
    
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    train_scores.append(train_score)
    test_scores.append(test_score)
    
    print(f"Degree {degree:2d}: Train R²={train_score:.4f}, Test R²={test_score:.4f}")

# Vẽ learning curve
plt.plot(degrees, train_scores, 'o-', label='Train Score')
plt.plot(degrees, test_scores, 's-', label='Test Score')
plt.xlabel('Polynomial Degree')
plt.ylabel('R² Score')
plt.legend()
plt.title('Model Complexity vs Performance')
plt.grid(True)
plt.show()
```

### 15.3. Sử Dụng Ridge Regression

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

# Pipeline với scaling và regularization
model = Pipeline([
    ('scaler', StandardScaler()),
    ('poly_features', PolynomialFeatures(degree=5)),
    ('ridge', Ridge(alpha=1.0))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"Ridge MSE: {mean_squared_error(y_test, y_pred):.4f}")
print(f"Ridge R²: {r2_score(y_test, y_pred):.4f}")
```

### 15.4. Grid Search Để Tìm Tham Số Tối Ưu

```python
from sklearn.model_selection import GridSearchCV

# Định nghĩa pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('poly_features', PolynomialFeatures()),
    ('ridge', Ridge())
])

# Định nghĩa grid tham số
param_grid = {
    'poly_features__degree': [1, 2, 3, 4, 5],
    'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]
}

# Grid search với cross-validation
grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=5, 
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score:", grid_search.best_score_)
print("Test score:", grid_search.score(X_test, y_test))
```

## 16. Best Practices

1. **Luôn bắt đầu đơn giản**: Thử Linear Regression trước, sau đó tăng dần độ phức tạp
2. **Sử dụng Cross-Validation**: Để chọn bậc n và tham số regularization
3. **Feature Scaling**: Luôn chuẩn hóa dữ liệu trước khi tạo polynomial features
4. **Regularization**: Sử dụng Ridge/Lasso với bậc cao để tránh overfitting
5. **Visualize**: Vẽ đồ thị để kiểm tra xem mô hình có hợp lý không
6. **Monitor overfitting**: So sánh train và test performance
7. **Domain knowledge**: Sử dụng kiến thức chuyên môn để chọn bậc phù hợp
8. **Avoid high degrees**: Thường không nên vượt quá degree=5 trừ khi có lý do rõ ràng

## 17. Tài Liệu Tham Khảo

- James, G., et al. (2013). An Introduction to Statistical Learning
- Hastie, T., et al. (2009). The Elements of Statistical Learning
- Scikit-learn Documentation: Polynomial Regression
- Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective

---

## Câu Hỏi Thường Gặp

**Q1: Polynomial Regression có phải là thuật toán phi tuyến không?**

A: Polynomial Regression là phi tuyến theo biến đầu vào x (vì có x², x³,...) nhưng vẫn là tuyến tính theo các hệ số w. Do đó, nó được gọi là "linear model" và có thể sử dụng các phương pháp tối ưu của Linear Regression.

**Q2: Làm sao biết bậc nào là phù hợp?**

A: Sử dụng cross-validation để thử nghiệm các bậc khác nhau và chọn bậc cho validation score tốt nhất. Cũng nên vẽ learning curves để quan sát overfitting. Thông thường, bậc 2-4 là phù hợp cho hầu hết các bài toán.

**Q3: Khi nào nên dùng Polynomial Regression thay vì Neural Networks?**

A: Dùng Polynomial Regression khi:
- Dữ liệu ít (< 1000 mẫu)
- Mối quan hệ có dạng đa thức đơn giản
- Cần mô hình dễ diễn giải
- Tài nguyên tính toán hạn chế
- Cần training nhanh

**Q4: Tại sao cần feature scaling cho Polynomial Regression?**

A: Vì các lũy thừa của x có thể tạo ra giá trị rất lớn (x¹⁰ với x=10 là 10 tỷ), gây ra numerical instability và làm gradient descent hội tụ chậm. Scaling đưa tất cả features về cùng một phạm vi.

**Q5: Có thể dùng Polynomial Regression cho classification không?**

A: Có thể kết hợp với Logistic Regression để tạo polynomial decision boundaries cho classification. Tạo polynomial features rồi áp dụng Logistic Regression lên các features đó.
