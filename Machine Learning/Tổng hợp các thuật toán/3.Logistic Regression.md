# Logistic Regression (Hồi Quy Logistic)

## 1. Giới Thiệu

Mặc dù có tên gọi là "Regression" (Hồi quy), Logistic Regression thực chất là một thuật toán phân loại (Classification), được sử dụng rộng rãi nhất cho bài toán phân loại nhị phân (Binary Classification). Đây là một trong những thuật toán cơ bản và quan trọng nhất trong Machine Learning, đặc biệt hữu ích khi cần hiểu xác suất của việc một mẫu thuộc về một lớp cụ thể.

## 2. Bản Chất

Logistic Regression được xây dựng dựa trên Linear Regression nhưng sử dụng hàm Sigmoid để chuyển đổi đầu ra thành xác suất trong khoảng [0, 1].

**Quy trình:**
1. Tính tổ hợp tuyến tính: z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ
2. Áp dụng hàm Sigmoid: P(y=1|x) = σ(z) = 1 / (1 + e⁻ᶻ)
3. Phân loại dựa trên ngưỡng (threshold)

**Sự khác biệt với Linear Regression:**

| Đặc điểm | Linear Regression | Logistic Regression |
|----------|-------------------|---------------------|
| Loại bài toán | Regression | Classification |
| Đầu ra | Giá trị liên tục (-∞, +∞) | Xác suất [0, 1] |
| Hàm kích hoạt | Không có | Sigmoid/Softmax |
| Hàm mất mát | MSE | Cross-Entropy |
| Ứng dụng | Dự đoán giá, nhiệt độ | Phân loại spam, bệnh |

## 3. Hàm Sigmoid

### 3.1. Công Thức

```
σ(z) = 1 / (1 + e⁻ᶻ)
```

Trong đó:
- **z**: Tổ hợp tuyến tính z = wᵀx + w₀
- **e**: Hằng số Euler (≈ 2.718)
- **σ(z)**: Xác suất đầu ra trong khoảng (0, 1)

### 3.2. Đặc Điểm

✓ **Miền giá trị**: (0, 1) - phù hợp để biểu diễn xác suất
✓ **Đối xứng**: σ(-z) = 1 - σ(z)
✓ **Điểm giữa**: σ(0) = 0.5
✓ **Tiệm cận**: 
  - z → +∞: σ(z) → 1
  - z → -∞: σ(z) → 0
✓ **Đạo hàm đơn giản**: σ'(z) = σ(z) × (1 - σ(z))

### 3.3. Ý Nghĩa

Hàm Sigmoid chuyển đổi bất kỳ giá trị thực nào thành xác suất:
- **z > 0**: Xác suất > 0.5 → Có xu hướng thuộc class 1
- **z = 0**: Xác suất = 0.5 → Không chắc chắn
- **z < 0**: Xác suất < 0.5 → Có xu hướng thuộc class 0

## 4. Phân Loại Nhị Phân (Binary Classification)

### 4.1. Công Thức Đầy Đủ

```
P(y=1|x) = σ(w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ)
P(y=0|x) = 1 - P(y=1|x)
```

### 4.2. Decision Boundary (Ranh Giới Quyết Định)

**Ngưỡng mặc định**: threshold = 0.5

```
ŷ = 1  nếu P(y=1|x) ≥ 0.5  (tương đương z ≥ 0)
ŷ = 0  nếu P(y=1|x) < 0.5   (tương đương z < 0)
```

**Decision boundary** là đường/mặt phẳng nơi z = 0:
```
w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ = 0
```

### 4.3. Điều Chỉnh Threshold

Có thể thay đổi threshold tùy theo yêu cầu:


**Threshold thấp (< 0.5)**: Tăng Recall, giảm Precision (ưu tiên phát hiện)
**Threshold cao (> 0.5)**: Tăng Precision, giảm Recall (ưu tiên chính xác)

### 4.4. Ví Dụ

**Phát hiện spam email:**
```
P(spam|email) = σ(w₀ + w₁×(số_từ_spam) + w₂×(số_link))
```
- P ≥ 0.5 → Spam
- P < 0.5 → Not Spam

## 5. Hàm Mất Mát (Loss Function)

### 5.1. Binary Cross-Entropy Loss

Còn gọi là Log Loss:

```
J(w) = -(1/n) Σᵢ₌₁ⁿ [yᵢ log(ŷᵢ) + (1-yᵢ) log(1-ŷᵢ)]
```

Trong đó:
- **yᵢ**: Nhãn thực tế (0 hoặc 1)
- **ŷᵢ**: Xác suất dự đoán P(y=1|xᵢ)
- **n**: Số lượng mẫu

### 5.2. Tại Sao Không Dùng MSE?

MSE không phù hợp vì:
- Tạo ra hàm mất mát không lồi (non-convex)
- Gradient descent khó hội tụ
- Không phản ánh tốt sai số phân loại

Cross-Entropy tạo ra hàm lồi, dễ tối ưu hóa.

### 5.3. Ý Nghĩa Cross-Entropy

**Khi y = 1:**
```
Loss = -log(ŷ)
```
- ŷ → 1: Loss → 0 (dự đoán đúng)
- ŷ → 0: Loss → ∞ (dự đoán sai hoàn toàn)

**Khi y = 0:**
```
Loss = -log(1-ŷ)
```
- ŷ → 0: Loss → 0 (dự đoán đúng)
- ŷ → 1: Loss → ∞ (dự đoán sai hoàn toàn)

## 6. Tối Ưu Hóa

### 6.1. Gradient Descent

Cập nhật trọng số theo gradient:

```
w := w - α × ∇J(w)
```

**Gradient của Cross-Entropy:**
```
∂J/∂wⱼ = (1/n) Σᵢ₌₁ⁿ (ŷᵢ - yᵢ) × xᵢⱼ
```

Công thức này giống Linear Regression nhưng ŷᵢ = σ(wᵀxᵢ).

### 6.2. Các Phương Pháp Tối Ưu

- **Batch Gradient Descent**: Sử dụng toàn bộ dữ liệu
- **Stochastic Gradient Descent (SGD)**: Từng mẫu một
- **Mini-batch Gradient Descent**: Batch nhỏ
- **Adam, RMSprop**: Adaptive learning rate
- **L-BFGS**: Quasi-Newton method (hiệu quả với dữ liệu nhỏ)

## 7. Regularization (Chính Quy Hóa)

### 7.1. L2 Regularization (Ridge)

```
J(w) = Cross-Entropy + λ Σwᵢ²
```

**Trong Scikit-learn:**
```python
LogisticRegression(penalty='l2', C=1.0)
```

Lưu ý: C = 1/λ (C càng nhỏ, regularization càng mạnh)

### 7.2. L1 Regularization (Lasso)

```
J(w) = Cross-Entropy + λ Σ|wᵢ|
```

**Đặc điểm:**
- Feature selection (đưa một số trọng số về 0)
- Tạo mô hình sparse

```python
LogisticRegression(penalty='l1', solver='liblinear', C=1.0)
```

### 7.3. Elastic Net

Kết hợp L1 và L2:

```python
LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)
```

### 7.4. Khi Nào Dùng Regularization?

✓ Khi có nhiều features
✓ Khi nghi ngờ overfitting
✓ Khi features có multicollinearity
✓ Khi muốn feature selection (dùng L1)

## 8. Multinomial Logistic Regression

### 8.1. Mở Rộng Cho Nhiều Lớp

Với K lớp, tính K scores:

```
z₁ = w₁ᵀx + b₁
z₂ = w₂ᵀx + b₂
...
zₖ = wₖᵀx + bₖ
```

### 8.2. Hàm Softmax

Chuyển đổi scores thành xác suất:

```
P(y=k|x) = exp(zₖ) / Σⱼ₌₁ᴷ exp(zⱼ)
```

**Đặc điểm:**
- Tất cả xác suất trong khoảng (0, 1)
- Tổng các xác suất = 1
- Lớp có xác suất cao nhất được chọn

### 8.3. Categorical Cross-Entropy

```
J(w) = -(1/n) Σᵢ₌₁ⁿ Σₖ₌₁ᴷ yᵢₖ log(ŷᵢₖ)
```

Trong đó yᵢₖ là one-hot encoding của nhãn.

### 8.4. Ví Dụ

**Phân loại hoa Iris (3 loại):**
```python
LogisticRegression(multi_class='multinomial', solver='lbfgs')
```

## 9. Đánh Giá Mô Hình

### 9.1. Confusion Matrix

```
                Predicted
              0         1
Actual  0    TN        FP
        1    FN        TP
```

- **TP** (True Positive): Dự đoán 1, thực tế 1
- **TN** (True Negative): Dự đoán 0, thực tế 0
- **FP** (False Positive): Dự đoán 1, thực tế 0 (Type I Error)
- **FN** (False Negative): Dự đoán 0, thực tế 1 (Type II Error)

### 9.2. Metrics

**Accuracy (Độ chính xác):**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Precision (Độ chính xác dương):**
```
Precision = TP / (TP + FP)
```
Trong số dự đoán positive, bao nhiêu đúng?

**Recall (Độ nhạy/Sensitivity):**
```
Recall = TP / (TP + FN)
```
Trong số thực tế positive, bao nhiêu được phát hiện?

**F1-Score:**
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```
Trung bình điều hòa của Precision và Recall.

**Specificity:**
```
Specificity = TN / (TN + FP)
```
Trong số thực tế negative, bao nhiêu được phát hiện đúng?

### 9.3. ROC Curve và AUC

**ROC Curve**: Đồ thị True Positive Rate vs False Positive Rate

```
TPR = Recall = TP / (TP + FN)
FPR = FP / (FP + TN)
```

**AUC (Area Under Curve)**: Diện tích dưới ROC curve
- AUC = 1.0: Mô hình hoàn hảo
- AUC = 0.5: Mô hình ngẫu nhiên
- AUC < 0.5: Mô hình tệ hơn ngẫu nhiên

### 9.4. Precision-Recall Curve

Hữu ích khi dữ liệu imbalanced (mất cân bằng).

**Average Precision (AP)**: Diện tích dưới PR curve

## 10. Xử Lý Dữ Liệu Imbalanced

### 10.1. Vấn Đề

Khi một class chiếm đa số (ví dụ: 95% negative, 5% positive):
- Mô hình có xu hướng dự đoán class đa số
- Accuracy cao nhưng không phản ánh hiệu suất thực

### 10.2. Giải Pháp

**1. Class Weights:**
```python
LogisticRegression(class_weight='balanced')
```
Tự động điều chỉnh trọng số: w_class = n_samples / (n_classes × n_samples_class)

**2. Resampling:**
- **Oversampling**: Tăng số lượng mẫu minority class (SMOTE)
- **Undersampling**: Giảm số lượng mẫu majority class

**3. Điều Chỉnh Threshold:**
Giảm threshold để tăng recall cho minority class.

**4. Sử Dụng Metrics Phù Hợp:**
- F1-Score thay vì Accuracy
- Precision-Recall AUC thay vì ROC AUC

## 11. Giả Định và Yêu Cầu

### 11.1. Giả Định

1. **Độc lập**: Các quan sát độc lập với nhau
2. **Tuyến tính**: Mối quan hệ tuyến tính giữa log-odds và features
3. **Không multicollinearity**: Features không tương quan cao
4. **Dữ liệu đủ lớn**: Cần đủ mẫu cho mỗi class

### 11.2. Kiểm Tra Giả Định

- **VIF (Variance Inflation Factor)**: Kiểm tra multicollinearity
- **Residual plots**: Kiểm tra tuyến tính
- **Sample size**: Ít nhất 10-20 mẫu cho mỗi feature

## 12. Ưu Điểm

✓ Đơn giản, dễ hiểu và triển khai
✓ Hiệu quả với dữ liệu tuyến tính phân tách được
✓ Cung cấp xác suất, không chỉ nhãn
✓ Dễ diễn giải (interpretability cao)
✓ Training nhanh, ít tài nguyên
✓ Hoạt động tốt với dữ liệu high-dimensional
✓ Không cần feature scaling (nhưng nên làm)
✓ Ít bị overfitting với regularization
✓ Có thể mở rộng cho multi-class

## 13. Nhược Điểm

✗ Chỉ mô hình hóa được decision boundary tuyến tính
✗ Không phù hợp với dữ liệu phi tuyến phức tạp
✗ Giả định tuyến tính giữa log-odds và features
✗ Nhạy cảm với outliers
✗ Yêu cầu features độc lập (multicollinearity gây vấn đề)
✗ Cần nhiều dữ liệu cho mỗi class
✗ Không tự động tạo feature interactions

## 14. Ví Dụ Thực Tế

### Ví dụ 1: Phát hiện spam email
```
P(spam) = σ(w₀ + w₁×từ_spam + w₂×số_link + w₃×độ_dài)
```

### Ví dụ 2: Chẩn đoán bệnh
```
P(bệnh) = σ(w₀ + w₁×tuổi + w₂×huyết_áp + w₃×cholesterol)
```

### Ví dụ 3: Dự đoán khách hàng rời bỏ (Churn)
```
P(churn) = σ(w₀ + w₁×thời_gian_sử_dụng + w₂×số_khiếu_nại)
```

### Ví dụ 4: Phê duyệt khoản vay
```
P(phê_duyệt) = σ(w₀ + w₁×thu_nhập + w₂×điểm_tín_dụng)
```

## 15. Triển Khai Với Python

### 15.1. Binary Classification Cơ Bản

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt

# Tạo dữ liệu mẫu
from sklearn.datasets import make_classification
X, y = make_classification(
    n_samples=1000, n_features=20, n_informative=15,
    n_redundant=5, random_state=42
)

# Chia dữ liệu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Chuẩn hóa dữ liệu
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Khởi tạo và huấn luyện mô hình
model = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)

model.fit(X_train_scaled, y_train)

# Dự đoán
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

# Đánh giá
print("=== Model Performance ===")
print(f"Accuracy:  {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score:  {f1_score(y_test, y_pred):.4f}")
print(f"ROC AUC:   {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred))

print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

# Trọng số features
print("\n=== Feature Importance (Coefficients) ===")
feature_importance = pd.DataFrame({
    'feature': [f'Feature_{i}' for i in range(X.shape[1])],
    'coefficient': model.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)
print(feature_importance.head(10))
```

### 15.2. Vẽ ROC Curve

```python
# Tính ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Vẽ đồ thị
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, 
         label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
         label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()
```

### 15.3. Điều Chỉnh Threshold

```python
from sklearn.metrics import precision_recall_curve

# Tính precision-recall cho các threshold khác nhau
precisions, recalls, thresholds = precision_recall_curve(
    y_test, y_pred_proba
)

# Tìm threshold tối ưu (F1 cao nhất)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
optimal_idx = np.argmax(f1_scores[:-1])
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold: {optimal_threshold:.4f}")
print(f"F1-Score at optimal threshold: {f1_scores[optimal_idx]:.4f}")

# Dự đoán với threshold mới
y_pred_custom = (y_pred_proba >= optimal_threshold).astype(int)
print(f"\nF1-Score with custom threshold: {f1_score(y_test, y_pred_custom):.4f}")
```

### 15.4. Multinomial Classification

```python
from sklearn.datasets import load_iris

# Load dữ liệu Iris (3 classes)
iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Chuẩn hóa
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Multinomial Logistic Regression
model = LogisticRegression(
    multi_class='multinomial',
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)

model.fit(X_train_scaled, y_train)

# Dự đoán
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)

# Đánh giá
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred, 
                          target_names=iris.target_names))
```

### 15.5. Xử Lý Imbalanced Data

```python
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification

# Tạo dữ liệu imbalanced (10:1 ratio)
X, y = make_classification(
    n_samples=1000, n_features=20,
    weights=[0.9, 0.1], random_state=42
)

print(f"Original class distribution: {np.bincount(y)}")

# Chia dữ liệu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Phương pháp 1: Class weights
model_weighted = LogisticRegression(
    class_weight='balanced',
    random_state=42
)
model_weighted.fit(X_train, y_train)
y_pred_weighted = model_weighted.predict(X_test)

print("\n=== With Class Weights ===")
print(f"F1-Score: {f1_score(y_test, y_pred_weighted):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_weighted):.4f}")

# Phương pháp 2: SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"\nResampled class distribution: {np.bincount(y_train_resampled)}")

model_smote = LogisticRegression(random_state=42)
model_smote.fit(X_train_resampled, y_train_resampled)
y_pred_smote = model_smote.predict(X_test)

print("\n=== With SMOTE ===")
print(f"F1-Score: {f1_score(y_test, y_pred_smote):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_smote):.4f}")
```

### 15.6. Cross-Validation và Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV, cross_val_score

# Định nghĩa grid tham số
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga']
}

# Grid search với cross-validation
grid_search = GridSearchCV(
    LogisticRegression(max_iter=1000, random_state=42),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score:", grid_search.best_score_)

# Đánh giá trên test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_scaled)
print(f"Test F1-Score: {f1_score(y_test, y_pred):.4f}")
```

## 16. So Sánh Với Các Thuật Toán Khác

| Thuật toán | Ưu điểm | Nhược điểm | Khi nào dùng |
|------------|---------|------------|--------------|
| Logistic Regression | Đơn giản, nhanh, interpretable | Chỉ tuyến tính | Baseline, dữ liệu tuyến tính |
| Decision Tree | Phi tuyến, dễ hiểu | Dễ overfit | Dữ liệu phi tuyến, cần interpretability |
| Random Forest | Mạnh, ít overfit | Chậm, khó diễn giải | Dữ liệu phức tạp, cần accuracy cao |
| SVM | Hiệu quả với high-dim | Chậm với dữ liệu lớn | Dữ liệu phức tạp, ít mẫu |
| Neural Networks | Rất mạnh, phi tuyến | Cần nhiều dữ liệu, chậm | Dữ liệu rất lớn, phức tạp |
| Naive Bayes | Rất nhanh | Giả định độc lập mạnh | Text classification, real-time |

## 17. Best Practices

1. **Feature Engineering**: Tạo features tốt quan trọng hơn thuật toán
2. **Feature Scaling**: Luôn chuẩn hóa dữ liệu (StandardScaler)
3. **Handle Missing Values**: Xử lý giá trị thiếu trước khi training
4. **Regularization**: Sử dụng để tránh overfitting
5. **Cross-Validation**: Đánh giá hiệu suất tổng quát
6. **Imbalanced Data**: Sử dụng class weights hoặc resampling
7. **Threshold Tuning**: Điều chỉnh threshold theo business needs
8. **Feature Selection**: Loại bỏ features không quan trọng
9. **Interpretability**: Phân tích coefficients để hiểu mô hình
10. **Monitoring**: Theo dõi performance trên production data

## 18. Khi Nào Sử Dụng Logistic Regression?

✓ Khi cần mô hình đơn giản, dễ diễn giải
✓ Khi cần xác suất, không chỉ nhãn
✓ Khi dữ liệu có decision boundary tuyến tính
✓ Làm baseline trước khi thử mô hình phức tạp
✓ Khi tài nguyên tính toán hạn chế
✓ Khi cần training và inference nhanh
✓ Khi có dữ liệu high-dimensional nhưng tuyến tính
✓ Khi cần hiểu ảnh hưởng của từng feature

✗ Không nên dùng khi:
- Decision boundary rất phi tuyến
- Có nhiều feature interactions phức tạp
- Cần mô hình rất chính xác (có thể thử ensemble methods)
- Dữ liệu có cấu trúc phức tạp (hình ảnh, text phức tạp)

## 19. Tài Liệu Tham Khảo

- Bishop, C. M. (2006). Pattern Recognition and Machine Learning
- Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective
- Hastie, T., et al. (2009). The Elements of Statistical Learning
- Scikit-learn Documentation: Logistic Regression
- Andrew Ng. Machine Learning Course (Coursera)

---

## Câu Hỏi Thường Gặp

**Q1: Tại sao gọi là "Regression" nhưng lại dùng cho Classification?**

A: Tên gọi xuất phát từ lịch sử. Logistic Regression sử dụng phương trình regression (tuyến tính) nhưng áp dụng hàm Sigmoid để chuyển đổi thành xác suất, từ đó phân loại. Về bản chất, nó dự đoán log-odds (một giá trị liên tục), sau đó chuyển thành nhãn rời rạc.

**Q2: Khi nào nên dùng Logistic Regression thay vì Decision Tree?**

A: Dùng Logistic Regression khi:
- Cần mô hình đơn giản, dễ diễn giải
- Decision boundary có vẻ tuyến tính
- Cần xác suất chính xác
- Muốn training nhanh
- Có ít dữ liệu (Decision Tree dễ overfit)

**Q3: Làm sao chọn giá trị C (regularization parameter)?**

A: Sử dụng Cross-Validation với GridSearchCV hoặc RandomizedSearchCV. Thử các giá trị: [0.001, 0.01, 0.1, 1, 10, 100]. C nhỏ = regularization mạnh (underfitting), C lớn = regularization yếu (overfitting).

**Q4: Có cần feature scaling cho Logistic Regression không?**

A: Không bắt buộc nhưng nên làm vì:
- Gradient descent hội tụ nhanh hơn
- Regularization hoạt động công bằng trên tất cả features
- Coefficients dễ so sánh và diễn giải hơn

**Q5: Làm sao xử lý categorical features?**

A: Sử dụng:
- **One-Hot Encoding**: Cho nominal categories (không có thứ tự)
- **Label Encoding**: Cho ordinal categories (có thứ tự)
- **Target Encoding**: Encode dựa trên target (cẩn thận overfitting)

```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(sparse=False, drop='first')
X_encoded = encoder.fit_transform(X_categorical)
```

**Q6: Logistic Regression có thể xử lý non-linear relationships không?**

A: Có thể bằng cách:
- Tạo polynomial features
- Tạo interaction features
- Feature engineering (log, sqrt, etc.)
- Nhưng nếu quá phức tạp, nên dùng thuật toán khác (SVM với kernel, Neural Networks)
