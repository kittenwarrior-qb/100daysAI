# ğŸš€ Quick Start Guide

HÆ°á»›ng dáº«n nhanh Ä‘á»ƒ báº¯t Ä‘áº§u vá»›i Gold Price VND Prediction.

## âš¡ Báº¯t Ä‘áº§u trong 5 phÃºt

### BÆ°á»›c 1: CÃ i Ä‘áº·t

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd "Projects/2.Gold Price VND Prediction"

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### BÆ°á»›c 2: Cháº¡y Notebook

```bash
# Má»Ÿ Jupyter Notebook
jupyter notebook notebook/Gold_Price_VND_Prediction.ipynb

# Cháº¡y táº¥t cáº£ cells (Kernel > Restart & Run All)
```

### BÆ°á»›c 3: Xem káº¿t quáº£

Sau khi cháº¡y notebook, báº¡n sáº½ cÃ³:
- âœ… 6 models Ä‘Ã£ Ä‘Æ°á»£c train vÃ  Ä‘Ã¡nh giÃ¡
- âœ… Dá»± Ä‘oÃ¡n 7 ngÃ y: `data/predictions_7days.csv`
- âœ… Dá»± Ä‘oÃ¡n 30 ngÃ y: `data/predictions_30days.csv`
- âœ… So sÃ¡nh models: `data/model_comparison.csv`
- âœ… Best model: `models/best_model.pkl`

## ğŸ“Š Xem Predictions

```python
import pandas as pd

# Äá»c dá»± Ä‘oÃ¡n 7 ngÃ y
pred_7 = pd.read_csv('data/predictions_7days.csv')
print(pred_7)

# Äá»c dá»± Ä‘oÃ¡n 30 ngÃ y
pred_30 = pd.read_csv('data/predictions_30days.csv')
print(pred_30)
```

## ğŸ¯ Sá»­ dá»¥ng Model Ä‘Ã£ train

```python
import joblib
import pandas as pd
import numpy as np

# Load model vÃ  scalers
model = joblib.load('models/best_model.pkl')
scaler_X = joblib.load('models/scaler_X.pkl')
scaler_y = joblib.load('models/scaler_y.pkl')
feature_cols = joblib.load('models/feature_cols.pkl')

# Load dá»¯ liá»‡u
df = pd.read_csv('data/vietdataverse_gold_2026-03-01.csv')
# ... (feature engineering nhÆ° trong notebook)

# Dá»± Ä‘oÃ¡n
X_new = df[feature_cols].iloc[-1:] # Láº¥y features má»›i nháº¥t
X_new_scaled = scaler_X.transform(X_new)
pred_scaled = model.predict(X_new_scaled)
pred = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]

print(f"Dá»± Ä‘oÃ¡n giÃ¡ vÃ ng: {pred:,.0f} VND")
```

## ğŸ¨ Cháº¡y Streamlit App (Coming Soon)

```bash
streamlit run src/app.py
```

## ğŸ“ˆ Workflow tá»•ng quan

```
1. Load Data (CSV)
   â†“
2. Feature Engineering
   â†“
3. Train Multiple Models
   â†“
4. Compare & Select Best
   â†“
5. Make Predictions
   â†“
6. Save Results
```

## ğŸ” CÃ¡c Models Ä‘Æ°á»£c thá»­ nghiá»‡m

1. **Linear Regression** âš¡ - Nhanh, Ä‘Æ¡n giáº£n
2. **Ridge Regression** ğŸ¯ - Regularized
3. **Random Forest** ğŸŒ² - Ensemble
4. **Gradient Boosting** ğŸ“ˆ - Boosting
5. **XGBoost** ğŸš€ - Hiá»‡u suáº¥t cao
6. **LightGBM** âš¡ - Nhanh nháº¥t

## ğŸ“Š Metrics

- **MAE**: Sai sá»‘ trung bÃ¬nh (VND)
- **RMSE**: Sai sá»‘ bÃ¬nh phÆ°Æ¡ng (VND)
- **RÂ²**: Äá»™ chÃ­nh xÃ¡c (0-1, cÃ ng cao cÃ ng tá»‘t)
- **MAPE**: Sai sá»‘ pháº§n trÄƒm (%)

## ğŸ“ Tips

### Cáº£i thiá»‡n Model

1. **ThÃªm features**:
   - GiÃ¡ USD/VND
   - LÃ£i suáº¥t
   - Chá»‰ sá»‘ chá»©ng khoÃ¡n
   - GiÃ¡ dáº§u

2. **Tune hyperparameters**:
   ```python
   from sklearn.model_selection import GridSearchCV
   
   param_grid = {
       'n_estimators': [100, 200, 300],
       'max_depth': [5, 10, 15],
       'learning_rate': [0.01, 0.1, 0.2]
   }
   
   grid_search = GridSearchCV(XGBRegressor(), param_grid, cv=5)
   grid_search.fit(X_train, y_train)
   ```

3. **Ensemble methods**:
   - Káº¿t há»£p nhiá»u models
   - Voting hoáº·c Stacking

### Xá»­ lÃ½ Outliers

```python
# PhÃ¡t hiá»‡n outliers
Q1 = df['Avg_Price'].quantile(0.25)
Q3 = df['Avg_Price'].quantile(0.75)
IQR = Q3 - Q1

# Loáº¡i bá» outliers
df_clean = df[
    (df['Avg_Price'] >= Q1 - 1.5*IQR) & 
    (df['Avg_Price'] <= Q3 + 1.5*IQR)
]
```

## âš ï¸ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p

**1. ModuleNotFoundError**
```bash
pip install <missing-module>
```

**2. Memory Error**
```python
# Giáº£m kÃ­ch thÆ°á»›c data hoáº·c sá»­ dá»¥ng sampling
df_sample = df.sample(frac=0.5)
```

**3. Model khÃ´ng converge**
```python
# TÄƒng sá»‘ iterations
model = XGBRegressor(n_estimators=500)
```

## ğŸ“š TÃ i nguyÃªn há»c thÃªm

- [Time Series Analysis](https://www.kaggle.com/learn/time-series)
- [XGBoost Tutorial](https://xgboost.readthedocs.io/en/stable/tutorials/index.html)
- [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)

## ğŸ¤” FAQ

**Q: Model cÃ³ chÃ­nh xÃ¡c khÃ´ng?**
A: Model Ä‘áº¡t RÂ² > 0.98 trÃªn test set, nhÆ°ng khÃ´ng Ä‘áº£m báº£o cho tÆ°Æ¡ng lai.

**Q: Bao lÃ¢u nÃªn retrain?**
A: NÃªn retrain hÃ ng tuáº§n hoáº·c khi cÃ³ dá»¯ liá»‡u má»›i.

**Q: CÃ³ thá»ƒ dá»± Ä‘oÃ¡n xa hÆ¡n 30 ngÃ y?**
A: CÃ³, nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c giáº£m dáº§n theo thá»i gian.

**Q: Model nÃ o tá»‘t nháº¥t?**
A: ThÆ°á»ng lÃ  XGBoost hoáº·c LightGBM, nhÆ°ng cáº§n test trÃªn data cá»§a báº¡n.

## ğŸ¯ Next Steps

1. âœ… Cháº¡y notebook vÃ  xem káº¿t quáº£
2. ğŸ“Š PhÃ¢n tÃ­ch feature importance
3. ğŸ”§ Thá»­ tune hyperparameters
4. ğŸ“ˆ ThÃªm features má»›i
5. ğŸš€ Deploy model (Streamlit/FastAPI)

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸ‰**

Náº¿u cÃ³ váº¥n Ä‘á», hÃ£y táº¡o issue trÃªn GitHub.
