# So sánh các Machine Learning Models

## Mục lục
1. [Naive Bayes](#naive-bayes)
2. [Logistic Regression](#logistic-regression)
3. [Support Vector Machine (SVM)](#svm)
4. [Decision Tree](#decision-tree)
5. [Random Forest](#random-forest)
6. [K-Nearest Neighbors (KNN)](#knn)
7. [Neural Networks](#neural-networks)
8. [Cách chọn model phù hợp](#cách-chọn-model)

---

## 1. Naive Bayes {#naive-bayes}

### Nguồn gốc
- Dựa trên **Định lý Bayes** (Thomas Bayes, thế kỷ 18)
- "Naive" vì giả định các features độc lập với nhau
- Phổ biến từ những năm 1960s trong text classification

### Cách hoạt động
Tính xác suất có điều kiện: P(Class|Features) = P(Features|Class) × P(Class) / P(Features)

### Ưu điểm
- **Cực kỳ nhanh** trong training và prediction
- Hoạt động tốt với **dữ liệu nhỏ**
- Hiệu quả với **text classification** (spam filter, sentiment analysis)
- Không cần nhiều data để train
- Xử lý tốt với **high-dimensional data**

### Nhược điểm
- Giả định features độc lập (thường không đúng trong thực tế)
- Không học được mối quan hệ phức tạp giữa features
- Kém với continuous features

### Khi nào dùng
- Text classification (spam, sentiment)
- Dữ liệu nhỏ, cần train nhanh
- Baseline model để so sánh
- Real-time prediction

---

## 2. Logistic Regression {#logistic-regression}

### Nguồn gốc
- Phát triển bởi David Cox năm 1958
- Mặc dù tên là "Regression" nhưng dùng cho **Classification**
- Một trong những thuật toán cơ bản nhất trong ML

### Cách hoạt động
- Sử dụng **sigmoid function** để chuyển output thành xác suất (0-1)
- Tìm đường thẳng (hoặc hyperplane) tách 2 class
- Công thức: σ(z) = 1 / (1 + e^(-z))

### Ưu điểm
- **Dễ hiểu và giải thích** (interpretable)
- Training nhanh
- Cho output là **xác suất** (không chỉ class)
- Ít bị overfit với regularization (L1, L2)
- Hoạt động tốt với **linearly separable data**
- Có thể mở rộng cho multi-class (softmax)

### Nhược điểm
- Chỉ học được **linear decision boundary**
- Không tốt với dữ liệu phức tạp, non-linear
- Nhạy cảm với outliers
- Cần feature scaling

### Khi nào dùng
- Cần model đơn giản, dễ giải thích
- Binary classification
- Cần biết xác suất của prediction
- Dữ liệu có thể tách tuyến tính
- Baseline model tốt

---

## 3. Support Vector Machine (SVM) {#svm}

### Nguồn gốc
- Phát triển bởi Vladimir Vapnik và Alexey Chervonenkis (1963)
- Phổ biến từ những năm 1990s
- Dựa trên lý thuyết **Statistical Learning Theory**

### Cách hoạt động
- Tìm **hyperplane tốt nhất** để tách các class
- Tối đa hóa **margin** (khoảng cách) giữa các class
- Sử dụng **kernel trick** để xử lý non-linear data

### Ưu điểm
- Hiệu quả với **high-dimensional data**
- Hoạt động tốt với **small to medium datasets**
- Có thể xử lý **non-linear** data với kernels (RBF, polynomial)
- Ít bị overfit trong high-dimensional space
- Memory efficient (chỉ dùng support vectors)

### Nhược điểm
- **Chậm** với dataset lớn (>10,000 samples)
- Khó tune hyperparameters (C, gamma, kernel)
- Không cho xác suất trực tiếp
- Khó giải thích
- Nhạy cảm với feature scaling

### Khi nào dùng
- Dataset vừa và nhỏ (<10k samples)
- High-dimensional data (text, images)
- Cần accuracy cao
- Data có thể non-linear

---

## 4. Decision Tree {#decision-tree}

### Nguồn gốc
- Phát triển từ những năm 1960s
- CART (Classification and Regression Trees) - Breiman et al. 1984
- ID3, C4.5 - Ross Quinlan

### Cách hoạt động
- Chia data thành các nhánh dựa trên features
- Mỗi node là một câu hỏi về feature
- Leaf nodes là predictions

### Ưu điểm
- **Cực kỳ dễ hiểu và visualize**
- Không cần feature scaling
- Xử lý được cả numerical và categorical data
- Tự động feature selection
- Xử lý được non-linear relationships
- Nhanh trong prediction

### Nhược điểm
- **Dễ overfit** (học thuộc training data)
- Không stable (data thay đổi chút → tree khác hẳn)
- Bias với imbalanced data
- Không tốt với linear relationships

### Khi nào dùng
- Cần model dễ giải thích
- Exploratory analysis
- Làm base cho ensemble methods
- Mixed data types

---

## 5. Random Forest {#random-forest}

### Nguồn gốc
- Phát triển bởi Leo Breiman năm 2001
- Là **ensemble** của nhiều Decision Trees
- Dựa trên ý tưởng "wisdom of crowds"

### Cách hoạt động
- Train nhiều Decision Trees trên random subsets của data
- Mỗi tree vote, kết quả cuối là majority vote
- Bagging + Random feature selection

### Ưu điểm
- **Accuracy cao**, ít overfit hơn Decision Tree
- Hoạt động tốt với **default parameters**
- Xử lý được missing values
- Cho **feature importance**
- Không cần feature scaling
- Xử lý tốt cả classification và regression
- Robust với outliers và noise

### Nhược điểm
- **Chậm** với dataset lớn
- Model size lớn (nhiều trees)
- Khó giải thích hơn single tree
- Có thể overfit với noisy data
- Không tốt với very high-dimensional sparse data (text)

### Khi nào dùng
- Cần accuracy cao
- Tabular data (structured data)
- Không muốn tune nhiều hyperparameters
- Cần feature importance
- Production-ready model

---

## 6. K-Nearest Neighbors (KNN) {#knn}

### Nguồn gốc
- Một trong những thuật toán ML đơn giản nhất
- Phát triển từ những năm 1950s
- "Lazy learning" - không có training phase

### Cách hoạt động
- Tìm K điểm gần nhất với điểm cần predict
- Vote theo majority (classification) hoặc average (regression)
- Dùng distance metrics (Euclidean, Manhattan, etc.)

### Ưu điểm
- **Cực kỳ đơn giản** để implement
- Không có training phase
- Tự nhiên xử lý multi-class
- Có thể học non-linear decision boundaries

### Nhược điểm
- **Cực kỳ chậm** trong prediction (phải tính distance với tất cả points)
- Cần nhiều memory (lưu toàn bộ training data)
- Nhạy cảm với **feature scaling**
- Không tốt với high-dimensional data (curse of dimensionality)
- Nhạy cảm với noisy data và outliers

### Khi nào dùng
- Dataset nhỏ
- Cần baseline đơn giản
- Không có thời gian train
- Low-dimensional data

---

## 7. Neural Networks / Deep Learning {#neural-networks}

### Nguồn gốc
- Perceptron - Frank Rosenblatt (1958)
- Backpropagation - 1980s
- Deep Learning boom - 2012 (AlexNet)

### Cách hoạt động
- Nhiều layers của neurons kết nối với nhau
- Mỗi connection có weight
- Học bằng backpropagation và gradient descent

### Ưu điểm
- **State-of-the-art** cho nhiều tasks phức tạp
- Tự động **feature learning**
- Xử lý được dữ liệu cực kỳ phức tạp (images, audio, text)
- Scalable với big data
- Transfer learning

### Nhược điểm
- Cần **rất nhiều data**
- Cần **GPU** để train
- Training rất lâu
- **Black box** - khó giải thích
- Nhiều hyperparameters phải tune
- Dễ overfit với small data

### Khi nào dùng
- Big data (>100k samples)
- Unstructured data (images, text, audio)
- Có GPU
- Cần state-of-the-art performance
- Không cần giải thích model

---

## Cách chọn Model phù hợp {#cách-chọn-model}

### 1. Dựa vào kích thước dữ liệu

| Kích thước | Models phù hợp |
|------------|----------------|
| < 1,000 | Naive Bayes, Logistic Regression, KNN |
| 1k - 10k | Logistic Regression, SVM, Decision Tree |
| 10k - 100k | Random Forest, SVM, Gradient Boosting |
| > 100k | Random Forest, Neural Networks, XGBoost |

### 2. Dựa vào loại dữ liệu

| Loại dữ liệu | Models phù hợp |
|--------------|----------------|
| Text | Naive Bayes, Logistic Regression, SVM |
| Tabular | Random Forest, XGBoost, Logistic Regression |
| Images | CNN (Deep Learning) |
| Time Series | LSTM, RNN, ARIMA |
| Mixed types | Decision Tree, Random Forest |

### 3. Dựa vào yêu cầu

| Yêu cầu | Models phù hợp |
|---------|----------------|
| Interpretability | Logistic Regression, Decision Tree |
| Speed | Naive Bayes, Logistic Regression |
| Accuracy | Random Forest, XGBoost, Neural Networks |
| Probability | Logistic Regression, Naive Bayes |
| No feature scaling | Decision Tree, Random Forest |

### 4. Quy trình chọn model

```
1. Bắt đầu với BASELINE đơn giản:
   - Logistic Regression (classification)
   - Linear Regression (regression)
   
2. Thử các model phức tạp hơn:
   - Random Forest
   - XGBoost
   - SVM
   
3. Nếu có nhiều data và cần SOTA:
   - Neural Networks
   - Deep Learning
   
4. So sánh và chọn model tốt nhất
```


### Tips

- **Luôn bắt đầu đơn giản**: Logistic Regression hoặc Naive Bayes
- **So sánh nhiều models**: Dùng cross-validation
- **Không có "best model"**: Phụ thuộc vào data và yêu cầu
- **Feature engineering** quan trọng hơn model choice
- **Ensemble methods** thường cho kết quả tốt nhất

---

## Bảng tổng hợp nhanh

| Model | Speed | Accuracy | Interpretability | Data Size | Complexity |
|-------|-------|----------|------------------|-----------|------------|
| Naive Bayes | ⚡⚡⚡ | ⭐⭐ | ⭐⭐⭐ | Small | Low |
| Logistic Reg | ⚡⚡⚡ | ⭐⭐⭐ | ⭐⭐⭐ | Any | Low |
| SVM | ⚡⚡ | ⭐⭐⭐⭐ | ⭐ | Small-Med | Medium |
| Decision Tree | ⚡⚡⚡ | ⭐⭐ | ⭐⭐⭐ | Any | Low |
| Random Forest | ⚡⚡ | ⭐⭐⭐⭐ | ⭐⭐ | Med-Large | Medium |
| KNN | ⚡ | ⭐⭐ | ⭐⭐ | Small | Low |
| Neural Net | ⚡ | ⭐⭐⭐⭐⭐ | ⭐ | Large | High |
