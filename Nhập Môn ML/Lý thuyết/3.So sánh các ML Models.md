# So s√°nh c√°c Machine Learning Models

## üìö M·ª•c l·ª•c
1. [Naive Bayes](#naive-bayes)
2. [Logistic Regression](#logistic-regression)
3. [Support Vector Machine (SVM)](#svm)
4. [Decision Tree](#decision-tree)
5. [Random Forest](#random-forest)
6. [K-Nearest Neighbors (KNN)](#knn)
7. [Neural Networks](#neural-networks)
8. [C√°ch ch·ªçn model ph√π h·ª£p](#c√°ch-ch·ªçn-model)

---

## 1. Naive Bayes {#naive-bayes}

### üéØ Ngu·ªìn g·ªëc
- D·ª±a tr√™n **ƒê·ªãnh l√Ω Bayes** (Thomas Bayes, th·∫ø k·ª∑ 18)
- "Naive" v√¨ gi·∫£ ƒë·ªãnh c√°c features ƒë·ªôc l·∫≠p v·ªõi nhau
- Ph·ªï bi·∫øn t·ª´ nh·ªØng nƒÉm 1960s trong text classification

### ‚ö° C√°ch ho·∫°t ƒë·ªông
T√≠nh x√°c su·∫•t c√≥ ƒëi·ªÅu ki·ªán: P(Class|Features) = P(Features|Class) √ó P(Class) / P(Features)

### ‚úÖ ∆Øu ƒëi·ªÉm
- **C·ª±c k·ª≥ nhanh** trong training v√† prediction
- Ho·∫°t ƒë·ªông t·ªët v·ªõi **d·ªØ li·ªáu nh·ªè**
- Hi·ªáu qu·∫£ v·ªõi **text classification** (spam filter, sentiment analysis)
- Kh√¥ng c·∫ßn nhi·ªÅu data ƒë·ªÉ train
- X·ª≠ l√Ω t·ªët v·ªõi **high-dimensional data**

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- Gi·∫£ ƒë·ªãnh features ƒë·ªôc l·∫≠p (th∆∞·ªùng kh√¥ng ƒë√∫ng trong th·ª±c t·∫ø)
- Kh√¥ng h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá ph·ª©c t·∫°p gi·ªØa features
- K√©m v·ªõi continuous features

### üéØ Khi n√†o d√πng
- Text classification (spam, sentiment)
- D·ªØ li·ªáu nh·ªè, c·∫ßn train nhanh
- Baseline model ƒë·ªÉ so s√°nh
- Real-time prediction

---

## 2. Logistic Regression {#logistic-regression}

### üéØ Ngu·ªìn g·ªëc
- Ph√°t tri·ªÉn b·ªüi David Cox nƒÉm 1958
- M·∫∑c d√π t√™n l√† "Regression" nh∆∞ng d√πng cho **Classification**
- M·ªôt trong nh·ªØng thu·∫≠t to√°n c∆° b·∫£n nh·∫•t trong ML

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- S·ª≠ d·ª•ng **sigmoid function** ƒë·ªÉ chuy·ªÉn output th√†nh x√°c su·∫•t (0-1)
- T√¨m ƒë∆∞·ªùng th·∫≥ng (ho·∫∑c hyperplane) t√°ch 2 class
- C√¥ng th·ª©c: œÉ(z) = 1 / (1 + e^(-z))

### ‚úÖ ∆Øu ƒëi·ªÉm
- **D·ªÖ hi·ªÉu v√† gi·∫£i th√≠ch** (interpretable)
- Training nhanh
- Cho output l√† **x√°c su·∫•t** (kh√¥ng ch·ªâ class)
- √çt b·ªã overfit v·ªõi regularization (L1, L2)
- Ho·∫°t ƒë·ªông t·ªët v·ªõi **linearly separable data**
- C√≥ th·ªÉ m·ªü r·ªông cho multi-class (softmax)

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- Ch·ªâ h·ªçc ƒë∆∞·ª£c **linear decision boundary**
- Kh√¥ng t·ªët v·ªõi d·ªØ li·ªáu ph·ª©c t·∫°p, non-linear
- Nh·∫°y c·∫£m v·ªõi outliers
- C·∫ßn feature scaling

### üéØ Khi n√†o d√πng
- C·∫ßn model ƒë∆°n gi·∫£n, d·ªÖ gi·∫£i th√≠ch
- Binary classification
- C·∫ßn bi·∫øt x√°c su·∫•t c·ªßa prediction
- D·ªØ li·ªáu c√≥ th·ªÉ t√°ch tuy·∫øn t√≠nh
- Baseline model t·ªët

---

## 3. Support Vector Machine (SVM) {#svm}

### üéØ Ngu·ªìn g·ªëc
- Ph√°t tri·ªÉn b·ªüi Vladimir Vapnik v√† Alexey Chervonenkis (1963)
- Ph·ªï bi·∫øn t·ª´ nh·ªØng nƒÉm 1990s
- D·ª±a tr√™n l√Ω thuy·∫øt **Statistical Learning Theory**

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- T√¨m **hyperplane t·ªët nh·∫•t** ƒë·ªÉ t√°ch c√°c class
- T·ªëi ƒëa h√≥a **margin** (kho·∫£ng c√°ch) gi·ªØa c√°c class
- S·ª≠ d·ª•ng **kernel trick** ƒë·ªÉ x·ª≠ l√Ω non-linear data

### ‚úÖ ∆Øu ƒëi·ªÉm
- Hi·ªáu qu·∫£ v·ªõi **high-dimensional data**
- Ho·∫°t ƒë·ªông t·ªët v·ªõi **small to medium datasets**
- C√≥ th·ªÉ x·ª≠ l√Ω **non-linear** data v·ªõi kernels (RBF, polynomial)
- √çt b·ªã overfit trong high-dimensional space
- Memory efficient (ch·ªâ d√πng support vectors)

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- **Ch·∫≠m** v·ªõi dataset l·ªõn (>10,000 samples)
- Kh√≥ tune hyperparameters (C, gamma, kernel)
- Kh√¥ng cho x√°c su·∫•t tr·ª±c ti·∫øp
- Kh√≥ gi·∫£i th√≠ch
- Nh·∫°y c·∫£m v·ªõi feature scaling

### üéØ Khi n√†o d√πng
- Dataset v·ª´a v√† nh·ªè (<10k samples)
- High-dimensional data (text, images)
- C·∫ßn accuracy cao
- Data c√≥ th·ªÉ non-linear

---

## 4. Decision Tree {#decision-tree}

### üéØ Ngu·ªìn g·ªëc
- Ph√°t tri·ªÉn t·ª´ nh·ªØng nƒÉm 1960s
- CART (Classification and Regression Trees) - Breiman et al. 1984
- ID3, C4.5 - Ross Quinlan

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- Chia data th√†nh c√°c nh√°nh d·ª±a tr√™n features
- M·ªói node l√† m·ªôt c√¢u h·ªèi v·ªÅ feature
- Leaf nodes l√† predictions

### ‚úÖ ∆Øu ƒëi·ªÉm
- **C·ª±c k·ª≥ d·ªÖ hi·ªÉu v√† visualize**
- Kh√¥ng c·∫ßn feature scaling
- X·ª≠ l√Ω ƒë∆∞·ª£c c·∫£ numerical v√† categorical data
- T·ª± ƒë·ªông feature selection
- X·ª≠ l√Ω ƒë∆∞·ª£c non-linear relationships
- Nhanh trong prediction

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- **D·ªÖ overfit** (h·ªçc thu·ªôc training data)
- Kh√¥ng stable (data thay ƒë·ªïi ch√∫t ‚Üí tree kh√°c h·∫≥n)
- Bias v·ªõi imbalanced data
- Kh√¥ng t·ªët v·ªõi linear relationships

### üéØ Khi n√†o d√πng
- C·∫ßn model d·ªÖ gi·∫£i th√≠ch
- Exploratory analysis
- L√†m base cho ensemble methods
- Mixed data types

---

## 5. Random Forest {#random-forest}

### üéØ Ngu·ªìn g·ªëc
- Ph√°t tri·ªÉn b·ªüi Leo Breiman nƒÉm 2001
- L√† **ensemble** c·ªßa nhi·ªÅu Decision Trees
- D·ª±a tr√™n √Ω t∆∞·ªüng "wisdom of crowds"

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- Train nhi·ªÅu Decision Trees tr√™n random subsets c·ªßa data
- M·ªói tree vote, k·∫øt qu·∫£ cu·ªëi l√† majority vote
- Bagging + Random feature selection

### ‚úÖ ∆Øu ƒëi·ªÉm
- **Accuracy cao**, √≠t overfit h∆°n Decision Tree
- Ho·∫°t ƒë·ªông t·ªët v·ªõi **default parameters**
- X·ª≠ l√Ω ƒë∆∞·ª£c missing values
- Cho **feature importance**
- Kh√¥ng c·∫ßn feature scaling
- X·ª≠ l√Ω t·ªët c·∫£ classification v√† regression
- Robust v·ªõi outliers v√† noise

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- **Ch·∫≠m** v·ªõi dataset l·ªõn
- Model size l·ªõn (nhi·ªÅu trees)
- Kh√≥ gi·∫£i th√≠ch h∆°n single tree
- C√≥ th·ªÉ overfit v·ªõi noisy data
- Kh√¥ng t·ªët v·ªõi very high-dimensional sparse data (text)

### üéØ Khi n√†o d√πng
- C·∫ßn accuracy cao
- Tabular data (structured data)
- Kh√¥ng mu·ªën tune nhi·ªÅu hyperparameters
- C·∫ßn feature importance
- Production-ready model

---

## 6. K-Nearest Neighbors (KNN) {#knn}

### üéØ Ngu·ªìn g·ªëc
- M·ªôt trong nh·ªØng thu·∫≠t to√°n ML ƒë∆°n gi·∫£n nh·∫•t
- Ph√°t tri·ªÉn t·ª´ nh·ªØng nƒÉm 1950s
- "Lazy learning" - kh√¥ng c√≥ training phase

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- T√¨m K ƒëi·ªÉm g·∫ßn nh·∫•t v·ªõi ƒëi·ªÉm c·∫ßn predict
- Vote theo majority (classification) ho·∫∑c average (regression)
- D√πng distance metrics (Euclidean, Manhattan, etc.)

### ‚úÖ ∆Øu ƒëi·ªÉm
- **C·ª±c k·ª≥ ƒë∆°n gi·∫£n** ƒë·ªÉ implement
- Kh√¥ng c√≥ training phase
- T·ª± nhi√™n x·ª≠ l√Ω multi-class
- C√≥ th·ªÉ h·ªçc non-linear decision boundaries

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- **C·ª±c k·ª≥ ch·∫≠m** trong prediction (ph·∫£i t√≠nh distance v·ªõi t·∫•t c·∫£ points)
- C·∫ßn nhi·ªÅu memory (l∆∞u to√†n b·ªô training data)
- Nh·∫°y c·∫£m v·ªõi **feature scaling**
- Kh√¥ng t·ªët v·ªõi high-dimensional data (curse of dimensionality)
- Nh·∫°y c·∫£m v·ªõi noisy data v√† outliers

### üéØ Khi n√†o d√πng
- Dataset nh·ªè
- C·∫ßn baseline ƒë∆°n gi·∫£n
- Kh√¥ng c√≥ th·ªùi gian train
- Low-dimensional data

---

## 7. Neural Networks / Deep Learning {#neural-networks}

### üéØ Ngu·ªìn g·ªëc
- Perceptron - Frank Rosenblatt (1958)
- Backpropagation - 1980s
- Deep Learning boom - 2012 (AlexNet)

### ‚ö° C√°ch ho·∫°t ƒë·ªông
- Nhi·ªÅu layers c·ªßa neurons k·∫øt n·ªëi v·ªõi nhau
- M·ªói connection c√≥ weight
- H·ªçc b·∫±ng backpropagation v√† gradient descent

### ‚úÖ ∆Øu ƒëi·ªÉm
- **State-of-the-art** cho nhi·ªÅu tasks ph·ª©c t·∫°p
- T·ª± ƒë·ªông **feature learning**
- X·ª≠ l√Ω ƒë∆∞·ª£c d·ªØ li·ªáu c·ª±c k·ª≥ ph·ª©c t·∫°p (images, audio, text)
- Scalable v·ªõi big data
- Transfer learning

### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
- C·∫ßn **r·∫•t nhi·ªÅu data**
- C·∫ßn **GPU** ƒë·ªÉ train
- Training r·∫•t l√¢u
- **Black box** - kh√≥ gi·∫£i th√≠ch
- Nhi·ªÅu hyperparameters ph·∫£i tune
- D·ªÖ overfit v·ªõi small data

### üéØ Khi n√†o d√πng
- Big data (>100k samples)
- Unstructured data (images, text, audio)
- C√≥ GPU
- C·∫ßn state-of-the-art performance
- Kh√¥ng c·∫ßn gi·∫£i th√≠ch model

---

## üéØ C√°ch ch·ªçn Model ph√π h·ª£p {#c√°ch-ch·ªçn-model}

### 1. D·ª±a v√†o k√≠ch th∆∞·ªõc d·ªØ li·ªáu

| K√≠ch th∆∞·ªõc | Models ph√π h·ª£p |
|------------|----------------|
| < 1,000 | Naive Bayes, Logistic Regression, KNN |
| 1k - 10k | Logistic Regression, SVM, Decision Tree |
| 10k - 100k | Random Forest, SVM, Gradient Boosting |
| > 100k | Random Forest, Neural Networks, XGBoost |

### 2. D·ª±a v√†o lo·∫°i d·ªØ li·ªáu

| Lo·∫°i d·ªØ li·ªáu | Models ph√π h·ª£p |
|--------------|----------------|
| Text | Naive Bayes, Logistic Regression, SVM |
| Tabular | Random Forest, XGBoost, Logistic Regression |
| Images | CNN (Deep Learning) |
| Time Series | LSTM, RNN, ARIMA |
| Mixed types | Decision Tree, Random Forest |

### 3. D·ª±a v√†o y√™u c·∫ßu

| Y√™u c·∫ßu | Models ph√π h·ª£p |
|---------|----------------|
| Interpretability | Logistic Regression, Decision Tree |
| Speed | Naive Bayes, Logistic Regression |
| Accuracy | Random Forest, XGBoost, Neural Networks |
| Probability | Logistic Regression, Naive Bayes |
| No feature scaling | Decision Tree, Random Forest |

### 4. Quy tr√¨nh ch·ªçn model

```
1. B·∫Øt ƒë·∫ßu v·ªõi BASELINE ƒë∆°n gi·∫£n:
   - Logistic Regression (classification)
   - Linear Regression (regression)
   
2. Th·ª≠ c√°c model ph·ª©c t·∫°p h∆°n:
   - Random Forest
   - XGBoost
   - SVM
   
3. N·∫øu c√≥ nhi·ªÅu data v√† c·∫ßn SOTA:
   - Neural Networks
   - Deep Learning
   
4. So s√°nh v√† ch·ªçn model t·ªët nh·∫•t
```

### 5. Cho b√†i to√°n Sentiment Analysis (Comment Filter)

**Recommended order:**

1. **Naive Bayes** ‚≠ê (Baseline, nhanh)
2. **Logistic Regression** ‚≠ê‚≠ê (T·ªët, d·ªÖ gi·∫£i th√≠ch)
3. **SVM with RBF kernel** ‚≠ê‚≠ê‚≠ê (Accuracy cao h∆°n)
4. **Random Forest** (N·∫øu c√≥ th√™m features)
5. **LSTM/BERT** (N·∫øu c√≥ nhi·ªÅu data v√† GPU)

### üí° Tips

- **Lu√¥n b·∫Øt ƒë·∫ßu ƒë∆°n gi·∫£n**: Logistic Regression ho·∫∑c Naive Bayes
- **So s√°nh nhi·ªÅu models**: D√πng cross-validation
- **Kh√¥ng c√≥ "best model"**: Ph·ª• thu·ªôc v√†o data v√† y√™u c·∫ßu
- **Feature engineering** quan tr·ªçng h∆°n model choice
- **Ensemble methods** th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t

---

## üìä B·∫£ng t·ªïng h·ª£p nhanh

| Model | Speed | Accuracy | Interpretability | Data Size | Complexity |
|-------|-------|----------|------------------|-----------|------------|
| Naive Bayes | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Small | Low |
| Logistic Reg | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Any | Low |
| SVM | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | Small-Med | Medium |
| Decision Tree | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Any | Low |
| Random Forest | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Med-Large | Medium |
| KNN | ‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê | Small | Low |
| Neural Net | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | Large | High |
