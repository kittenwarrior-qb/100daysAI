# 4. So sánh các Vectorizers (Text to Numbers)

## Tổng quan
Vectorizers là các phương pháp chuyển đổi text thành số để máy tính có thể xử lý trong Machine Learning.

---

## 1. CountVectorizer (Bag of Words)

### Cách hoạt động
- Đếm số lần xuất hiện của mỗi từ trong văn bản
- Tạo ma trận với mỗi cột là một từ trong vocabulary

### Ví dụ
```python
from sklearn.feature_extraction.text import CountVectorizer

texts = ["Sản phẩm tốt", "Sản phẩm rất tốt", "Sản phẩm xấu"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# Kết quả: ma trận đếm từ
# [[1, 1, 0, 1, 0],  # "Sản phẩm tốt"
#  [1, 1, 1, 1, 0],  # "Sản phẩm rất tốt"
#  [1, 1, 0, 0, 1]]  # "Sản phẩm xấu"
```

### Ưu điểm
- Đơn giản, dễ hiểu
- Nhanh, hiệu quả với dataset nhỏ
- Phù hợp cho bài toán classification cơ bản

### Nhược điểm
- Không quan tâm đến thứ tự từ
- Không phân biệt từ quan trọng/không quan trọng
- Ma trận sparse (nhiều số 0)
- Không hiểu ngữ nghĩa

---

## 2. TfidfVectorizer (TF-IDF)

### Cách hoạt động
- TF (Term Frequency): Tần suất từ trong document
- IDF (Inverse Document Frequency): Độ hiếm của từ trong toàn bộ corpus
- TF-IDF = TF × IDF (từ xuất hiện nhiều nhưng hiếm → trọng số cao)

### Ví dụ
```python
from sklearn.feature_extraction.text import TfidfVectorizer

texts = ["Sản phẩm tốt", "Sản phẩm rất tốt", "Sản phẩm xấu"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# Kết quả: ma trận trọng số TF-IDF
# Từ "Sản phẩm" xuất hiện ở tất cả → trọng số thấp
# Từ "rất", "xấu" hiếm → trọng số cao
```

### Ưu điểm
- Phân biệt được từ quan trọng
- Giảm ảnh hưởng của từ phổ biến (the, is, a...)
- Hiệu quả hơn CountVectorizer trong hầu hết trường hợp
- Phổ biến nhất cho text classification

### Nhược điểm
- Vẫn không hiểu ngữ nghĩa
- Không quan tâm thứ tự từ
- Ma trận sparse

---

## 3. Word2Vec / Word Embeddings

### Cách hoạt động
- Học vector representation của từ dựa trên context
- Từ có nghĩa giống nhau → vector gần nhau
- Mỗi từ → vector dense (thường 100-300 chiều)

### Ví dụ
```python
from gensim.models import Word2Vec

sentences = [["sản", "phẩm", "tốt"], ["sản", "phẩm", "xấu"]]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# Vector của từ "tốt"
vector = model.wv['tốt']  # [0.23, -0.45, 0.67, ...]

# Từ tương tự
similar = model.wv.most_similar('tốt')  # ['hay', 'đẹp', 'ok', ...]
```

### Ưu điểm
- Hiểu được ngữ nghĩa (semantic meaning)
- Vector dense, không sparse
- Có thể tính similarity giữa các từ
- Transfer learning (dùng pre-trained model)

### Nhược điểm
- Cần dataset lớn để train tốt
- Chậm hơn TF-IDF
- Phức tạp hơn để implement
- Mỗi từ chỉ có 1 vector (không phân biệt nghĩa khác nhau)

---

## 4. BERT / Transformers

### Cách hoạt động
- Sử dụng attention mechanism
- Hiểu context 2 chiều (trước và sau từ)
- Pre-trained trên corpus khổng lồ
- Fine-tune cho task cụ thể

### Ví dụ
```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

text = "Sản phẩm rất tốt"
inputs = tokenizer(text, return_tensors='pt', padding=True)
outputs = model(**inputs)

# Vector representation
embeddings = outputs.last_hidden_state  # [1, seq_len, 768]
```

### Ưu điểm
- State-of-the-art accuracy
- Hiểu context và ngữ nghĩa sâu
- Phân biệt được nghĩa khác nhau của cùng 1 từ
- Pre-trained models sẵn có

### Nhược điểm
- Rất chậm và tốn tài nguyên
- Cần GPU để train/inference nhanh
- Phức tạp, khó debug
- Overkill cho bài toán đơn giản

---

## 5. HashingVectorizer

### Cách hoạt động
- Sử dụng hash function để map từ → index
- Không cần lưu vocabulary
- Fixed size output

### Ví dụ
```python
from sklearn.feature_extraction.text import HashingVectorizer

vectorizer = HashingVectorizer(n_features=1000)
X = vectorizer.transform(texts)
```

### Ưu điểm
- Rất nhanh
- Tiết kiệm memory (không lưu vocabulary)
- Scalable cho big data

### Nhược điểm
- Hash collision (2 từ khác nhau → cùng index)
- Không thể inverse transform
- Khó interpret

---

## So sánh tổng quan

| Vectorizer | Tốc độ | Accuracy | Ngữ nghĩa | Use Case |
|------------|--------|----------|-----------|----------|
| CountVectorizer | ⚡⚡⚡ | ⭐⭐ | ❌ | Bài toán đơn giản, dataset nhỏ |
| TfidfVectorizer | ⚡⚡⚡ | ⭐⭐⭐ | ❌ | Text classification phổ thông |
| Word2Vec | ⚡⚡ | ⭐⭐⭐⭐ | ✅ | Semantic search, similarity |
| BERT | ⚡ | ⭐⭐⭐⭐⭐ | ✅✅ | Production, high accuracy needed |
| HashingVectorizer | ⚡⚡⚡⚡ | ⭐⭐ | ❌ | Big data, streaming |

---

## Khi nào dùng cái gì?

### Dùng CountVectorizer/TfidfVectorizer khi:
- Bài toán đơn giản (spam detection, sentiment analysis cơ bản)
- Dataset nhỏ/trung bình
- Cần kết quả nhanh
- Tài nguyên hạn chế
- **→ Phù hợp cho học tập và prototype**

### Dùng Word2Vec/FastText khi:
- Cần hiểu semantic similarity
- Có dataset đủ lớn để train
- Bài toán recommendation, search
- Cần transfer learning

### Dùng BERT/Transformers khi:
- Cần accuracy cao nhất
- Có GPU và tài nguyên
- Production system quan trọng
- Dataset phức tạp (nhiều ngữ cảnh)

### Dùng HashingVectorizer khi:
- Big data, streaming data
- Memory hạn chế
- Không cần interpret model

---

## Code ví dụ so sánh

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Data
texts = ["comment 1", "comment 2", ...]
labels = [0, 1, ...]

X_train, X_test, y_train, y_test = train_test_split(texts, labels)

# Test 1: CountVectorizer
cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_test_cv = cv.transform(X_test)
model1 = MultinomialNB().fit(X_train_cv, y_train)
print(f"CountVectorizer accuracy: {accuracy_score(y_test, model1.predict(X_test_cv))}")

# Test 2: TfidfVectorizer
tfidf = TfidfVectorizer()
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)
model2 = MultinomialNB().fit(X_train_tfidf, y_train)
print(f"TfidfVectorizer accuracy: {accuracy_score(y_test, model2.predict(X_test_tfidf))}")
```

---

## Kết luận

- **Bắt đầu với TfidfVectorizer** - balance tốt giữa đơn giản và hiệu quả
- **Nâng cao lên Word2Vec** - khi cần semantic understanding
- **Chuyển sang BERT** - khi cần accuracy cao nhất và có tài nguyên
- Không có "best" vectorizer - tùy vào bài toán và constraints
